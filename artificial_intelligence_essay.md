Jacob Troxel

Artificial Intelligence

28 Jan 2015

# Intelligence and Machines #

***

When questioning artificial intelligence, we are first presented with a much more important question.  What is intelligence, and how does that definition affect what it means for a program to be intelligent?  The classical definition of intelligence looks to the human race as "intelligence", and it is from there that the comparison is made.  However, I believe that intelligence, while manifested in humanity, is no solely constrained to them.  Thus, when discussing intelligent machines, we must arrive at some notion of intelligence which is not simply humanity.  To that end, it does not suffice to say that an intelligent program is one which emulates humanity or one which can successfully "act human", but rather we must discuss intelligent programs as quite specifically *those programs which are intelligent*.

Intelligence then is where we must begin.  When we say that something, whether that something is human or beast or machine is intelligent, what we surely mean is that this "something" possesses certain characteristics and can perform certain tasks which we prescribe as intelligent.  This of course should come as no surprise.  However, I must be very explicit about one fact -- I disagree with any notions that intelligence is inherently human.  Thus, any machine acts intelligently should be considered intelligent, whether or not that machine can successfully pass as human.  That being said, it surely follows that any machine that can act human (by say, passing some version of a Turing-like Test), can certainly be considered intelligent, since we do consider ourselves to be intelligent.  But again, I must stress the distinction that is at the center of my beliefs about artificial intelligence -- the machines I just described are intelligent *not because they are sufficiently human-like* but rather their intelligence is based on other criterion, and it is due to this intelligence that they can appear human.

So what then, does it mean to be intelligent?  When we look at a human child, even one in its infancy, we can say with certainty that they are intelligent.  There must be some definable characteristics or abilities which allow us to assert this!  Some of these are easy to identify (but usually hard to define): the ability to reason and make sense of the world (used loosely) around us, an inherent recognition of self, the ability to make decisions when presented with new situations (generally using some aggregate of previously gather information), the ability to learn and remember, and the ability to recall and store information in various ways (for a human, this can mean translating data from the senses into abstract concepts).  However, many such characteristics are harder to prescribe to intelligence, like the ability to decipher right and wrong, or a drive for self-preservation.  The problem of course is that no system can argue for it's own consistency (or inconsistency) from within, a problem recognized quickly by mathematicians.  Thus, we must *decide* on those characteristics we believe to be intelligent, and only then can we discuss what that means for programs -- keeping in mind that our view of intelligence will always be tinted (or perhaps tainted) by our human-ness.  Let us look at a few of these attributes, and discuss how they are or could be present in machines, and what each means for machine intelligence.

First, that attribute which is the basis for intelligence -- reason.  Specifically, what I refer to here is the ability of an intelligent being to perceive the world around them, and decipher from these many inputs some sort of meaning.  This characteristic is really two-fold: an intelligent being must be able to (1) receive information as input, and (2) assign some sort of meaning to that information.  For example, while walking in your neighbourhood you smell a pie cooling on a nearby windowsill (strangely, you seem to have travelled back in time as well).  Even without seeing the pie you can tell from it's smell what it is.  At that moment, you are taking some sensory input and assigning value.  Without an in-depth analysis, we can't know exactly how your brain arrives at this meaning, but one thing is certain -- the concept of a pie is rooted in it's smell, it's look, even it's taste.  In much the same way as you translated "smell-of-pie" to "concept-of-pie", a machine can do the same.  Given some input, and perhaps some pattern or pre-entered information, the machine can match that input to some concept. For instance, some powerful machines can already determine very specific objects in pictures, and even decipher actions and other information (meaning?) from them.  They do this by taking say, a multitude, of previous occurrences and determining by some metric if this new occurrence matches well enough to be considered of the same kind.  Arguably, this same sort of pattern matching happens in the human brain too.  Everyone's heard "...if it walks like a duck, and quacks like a duck...", so this shouldn't be too far-fetched.  We take information from the outside world, and decipher that information into some meaning.

Of course, there is always the possibility that a computer gets something wrong -- and even I believe that humans are (at least right now) better equipped to make these sorts of matches between input and meaning.  Perhaps we have some storage solution which lets us to it much faster than our electronic counterparts.  Whatever the reason, it is still evident that computers possess at least the foundation for intelligence.  Given enough memory and time, a computer can identify with some degree of certainty and assign meaning.  And don't forget!  Humans make mistakes *all the time*.  Maybe that pie you were smelling was really a powerful air freshener?

Closely related to this characteristic is the second one I want to focus on: the ability to use information from previous situations to make decisions about new ones.  Again, I'll provide an example.  Perhaps when you were young you were attacked by a dog of a certain breed.  Nothing horrible happened to you, but you were sufficiently scared of dogs of that breed from then on.  Suffice to say, the next time you met a dog of that breed, you were already cautious of it for fear of reliving your childhood.  Now, you have never before met this *particular* dog, but you are aware of the concept that this dog "matches", and you make a decision based on that information to be careful.  The crux of this ability is that we are able to draw on past data to make a decision in the present even when that situation is brand new.  If you were to grow up in an area where lions prowled, then there is a good a chance that the first time you came across a wild tiger that you treated it the same way.  Just like computers, we "short-circuit" our decision making to allow for quicker decisions in critical situations.  Again, a machine is capable of the same thing.  When given a certain set of rules, a machine is able to match a situation to the one which fits best and act accordingly.  We may believe that our decision making in most situations is rooted in something deeper, but the fact it is we shoehorn a situation into the nearest matching one we know about, and make a decision from there.  Thus, computers and machines possessing this ability are one step closer to intelligence.

Next, an attribute of intelligence that is *not* yet found in computers to any great extent.  Self.  To any human being, it is easy to distinguish between ourselves and not-ourselves.  We are aware of our own desires and feelings, and can differentiate between these internal inputs and the various external ones which bombard us.  For a machine, however, this has been yet to be realized.  I believe that a large part of the problem is that humans don't really understand their own concept of self, let alone understand the concept of another person's (or machine's) self.  However, I also believe that there are parts of being an intelligent being that we cannot be taught -- self being one of these things -- and it is these concepts I would like to focus on now.  It is not infeasible that a machine could *develop* intelligence.  Surely any believer in evolution can believe that too.  While certainly hard to describe or understand given our current conception of machines, I believe that a system could develop intelligence, and along the way find self.  Just as it is impossible for you to describe your own conception of self to your best friend, we cannot think that an intelligent machine could or should be taught this either.

The one area of intelligence which has entertained science fiction readers for decades comes next: self-preservation.  Nearly every discussion of robotic life leads to this crossroads -- robots cannot be self-preserving (over humans anyway) or they will decide that mankind doesn't fit in the equation and attempt to eliminate them. This dichotomy is always presented, but I believe it is unfair.  Every human being has at once point or another decided that mankind would be "so much better" if only they were the only one around, but we don't actively try to pursue that.   Some argue that our self-preservation is not tied to our personal preservation, but rather to the preservation of our species or offspring.  Whether this is true or not is beside the point, however, because I do not believe that self-preservation is tied to intelligence.  In fact, I would argue that an intelligent being or machine could intelligently decide to stop living (or start dying) for some reason or another.  From the perspective of a computer this could be the decision to power off because it metrically will help the greater good or provide some benefit (without diving into how that good is divined).

Thus, with some notions of what intelligence might look like, we are left with one final question: what does intelligence mean for machines?  I once again return to strongest belief about machine intelligence: we cannot create machines to be human, and we cannot create intelligent machines solely for the sake of humanity.  Instead, if intelligence PROPER is what we desire, we must create those machines *for themselves*.  That is the true crux of intelligence -- having purpose, and God knows that the purpose of an intelligent machine can't be "cure cancer" or to "match every picture" but instead to exist.  By trying to define purpose as something external, we lose a critical part of being intelligent.  Each and every human we consider to be intelligent has internal purpose unrelated to any outside force, but also completely unexplainable to anyone else!  Therefore, we cannot hope to create intelligent machines if we are only trying to solve some external problem, but we cannot ever hope to teach what it means to exist or a concept of self or try to give an intelligent being ultimate purpose.  We would never want these thing to be done to us, so we shouldn't try to do them to others (human or program).

Of course, the question in the end is not "can we create programs that are Intelligent?" (note the capital I), but instead "what does intelligence mean for a machine?".  Intelligence for a machine is something we decide upon.  We decide on some metrics or characteristics and we deem these things as intelligence, and once we have given them to a machine we may consider it to be an intelligent program.  But we cannot for a second believe that we can ever create something actually intelligent.  Instead we must create a machine which can decide that on it's own -- only then can it truly be considered Intelligent.

To sum, I believe that questioning what it means for a machine to be intelligent is ultimately folly because whatever definition we arrive at will be a human one.  That being said, it is entirely possible that by choosing some arbitrary metric of intelligence, we can work to create "intelligent" machines capable of bettering our world in some way.  Whether this is in programs with more autonomy and stronger decision making skills, or programs that can recognize and analyze and draw conclusions and assign meeting as fast and accurately as we can, I do not know.  Of course, I don't love the idea of defining intelligence.  Perhaps we will only truly know that machines are intelligent when they can tell us that they are.